# UNIMI-nlp-project
Project repository for Natural Language Processing course, ay 23-24 - Riccardo Raffini

The code inside this repository is organized in two distinct flows: the first one regarding the training and use of an evolutionary algorithms framework and the second one regarding the training and use of a large language model (GPT2).<br>
Although, the flows are distinct, they share the same objective, that is generating new cooking recipes starting from a simple user input, such as a simple list of required/desired ingredients that the newly generated recipes should include.

## Datasets

The following dataset were employed in the train and evaluation processes of both methods:

- Food.com recipes([link](https://www.kaggle.com/datasets/realalexanderwei/food-com-recipes-with-ingredients-and-tags)) and reviews ([link](https://www.kaggle.com/datasets/irkaal/foodcom-recipes-and-reviews))
- Recipes3K ([link](https://www.kaggle.com/datasets/crispen5gar/recipes3k?select=recipes.json))

## Evolutionary algorithms

### Train

Evolutionary algorithms training process consists in an in-depth analysis of the relations occurring between ingredients and cooking actions, which involve a Name Entity Recognition model (BERT), so before starting the train of the main method, it is necessary to have a trained NER model to employ. Such model can be trained running the script [`ner/train.py`](ner/train.py), while the other scripts in the same folder can be used to create an annotated dataset.

Once the NER model is available, the main training script can be run as follows. This gernerates a collection of matrices storing the recipes relations.

```shell
python scripts/generate_ea_matrices.py
```

Since relation matrices computation is a very time-consuming task, a collection of matrices grouping relations extracted by 30.000 recipes is available in [`models/evolutionary_algorithms/`](models/evolutionary_algorithms/).

### Generate recipes

Once the relation matrices are available, new cooking recipes can be generated by running the file [`algorithms/generate.py`](algorithms/generate.py) with required arguments. An example of execution of the script is the following:

```shell
python .\algorithms\generate.py --relation_matrices_name='models/evolutionary_algorithms/global_recipes_matrices_30k'
```

Some parameters of the underlying evolutionary algorithm can be modified by adding options to the command, however to adjust other parameters it is necessary to edit the algorithm's configuration in the script.

## Large language model

### Train

In order to train the generative language model, it is necessary to create a compatible dataset made of tokenized recipes. The script [`scripts/generate_llms_dataset.py`](scripts/generate_llms_dataset.py) groups and execute all the functions necessary to create such dataset, starting from a compatible dataframe of raw examples.<br>
During the execution three files are created, one with extension `.h5` which represents the final dataset containing the tokenized examples and two others with extension `.txt` which are just intermediate helper files, that can be safely removed after the script is terminated.

Once a token dataset is available, the language model can be easily trained (and/or tested) by running the file [`llms/train.py`](llms/train.py) with required arguments. An example of the execution of the script is the following:

```shell
python llms/train.py --train --tokenized_dataset_path=recipes_tokenized_60k.h5
```

Testing of the model can be performed while training adding the flag `--eval_during_training` or it can be done sparately re-running the train script with the appropriate arguments:

```shell
python llms/train.py --eval --model_path=models/gpt2-finetuned-recipe-generation/checkpoint-final --tokenized_dataset_path=recipes_tokenized_60k.h5
```

### Generate recipes

Once the generative model is trained, new cooking recipes can be generated by running the file [`llms/generate.py`](llms/generate.py) or defining a similar process to the one described in it.<br>
As for the training, the generation file can be executed through a command as the following:

```shell
python .\llms\generate.py --model_path=models/gpt2-finetuned-recipe-generation/checkpoint-final
```

It is possible to either specify the user prompt (list of ingredients) in the command, using the `--prompt` option, or wait for the script to ask the user for it. Also note that if the generated recipe is too long, a failed generation message may appear, to solve this consider increasing the maximum recipe length by specifying the `--length` option.

Note that the output of the model, is still a sequence of text token that may require further processing, see the Textual representation section for more information about that.

## Textual representation

As expected both the given methods produce some sort of raw outputs (respectively a recipe graph and a sequence of tokens), althought are quite easily understandable, their readability can be improved by providing the output as a more structured text highlighting the ingredients and the instruction steps, as one would expect a cooking recipe to be.

The file [`commons/text.py`](commons/text.py) contains some classes that can be used to turn any recipe graph or tokens sequence outputted from the generative methods into a markdown text representation listing ingredients and instructions.
